1. 서브워드 기반 토크나이저
(1) BPE (Byte Pair Encoding)
사용 예: GPT, OpenAI 모델
원리: 가장 자주 나타나는 문자 쌍을 병합하여 서브워드 단위를 생성.
특징: 단순하고 빠르며, 언어에 독립적.
(2) WordPiece
사용 예: BERT, DistilBERT
원리: BPE와 유사하지만, 최대 확률을 기반으로 병합 후보를 선택.
특징: 서브워드 분할이 BPE보다 더 세분화될 가능성.
(3) SentencePiece
사용 예: T5, ALBERT
원리: 문장 레벨에서 처리하며, 공백도 문자로 취급.
특징: 언어에 종속되지 않으며 유니코드 처리가 강력.
(4) Unigram
사용 예: SentencePiece에서 지원.
원리: 미리 정의된 서브워드 집합 중 확률이 높은 조합을 선택.
특징: 모델이 단어 분할 경로를 여러 후보 중에서 선택 가능.

2. 단어 기반 토크나이저
(1) SpaCy
사용 예: NLP 파이프라인에서 널리 사용.
원리: 언어별 규칙과 통계 기반의 단어 분리.
특징: 빠르고 강력한 언어 지원.
(2) NLTK
사용 예: 연구 및 학습용 NLP 프로젝트.
원리: 정규표현식 기반.
특징: 간단하지만 실행 속도가 상대적으로 느림.

3. 문자 기반 토크나이저
(1) Character-level Tokenizer
사용 예: GPT, Seq2Seq 모델에서 일부 사용.
원리: 문자를 개별 토큰으로 처리.
특징: 언어에 독립적이며, 작은 모델에서 사용.

4. 토크나이저 플랫폼
(1) Hugging Face Tokenizers
사용 예: 다양한 NLP 모델(BERT, GPT 등).
원리: BPE, WordPiece, SentencePiece 등 다양한 토크나이징 방법 지원.
특징: Rust로 구현되어 매우 빠르며, Python과 통합 가능.
(2) FastText
사용 예: Facebook 연구팀이 개발한 단어 임베딩 학습.
원리: n-gram 기반.
특징: 단어 수준에서 서브워드 임베딩 지원.

5. 한국어 특화 토크나이저
(1) Mecab
사용 예: 한국어 NLP에서 가장 널리 사용.
특징: 형태소 분석에 강력하며 빠른 속도.
(2) Komoran
사용 예: 한국어 형태소 분석.
특징: 정확성이 높고, Java 기반의 높은 확장성.
(3) Okt (TwitterTokenizer)
사용 예: 간단한 한국어 텍스트 처리.
특징: Python에서 쉽게 사용할 수 있음.
(4) KoNLPy
사용 예: Mecab, Okt, Komoran 등 다양한 한국어 분석기와 통합 가능.
특징: 초보자부터 전문가까지 폭넓게 사용.