from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import ByteLevel  # âœ… ë°”ì´íŠ¸ ë ˆë²¨ í† í¬ë‚˜ì´ì € ì‚¬ìš©
import json
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = ByteLevel()  # âœ… ë°”ì´íŠ¸ ë‹¨ìœ„ í† í°í™” ì ìš©

trainer = BpeTrainer(
    vocab_size=30000,  # ì„œë¸Œì›Œë“œ ê°œìˆ˜
    min_frequency=2,    # ìµœì†Œ ë¹ˆë„
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
)

corpus_path = "wiki_corpus.txt"
tokenizer.train([corpus_path], trainer)

vocab = tokenizer.get_vocab()
merge = tokenizer.model.merges

with open("vocab.json", "w", encoding="utf-8") as vocab_file:
    json.dump(vocab, vocab_file, indent=2, ensure_ascii=False)

with open("merges.json", "w", encoding="utf-8") as merges_file:
    json.dump(merge, merges_file, indent=2, ensure_ascii=False)

tokenizer.save("bbpe_tokenizer.bin")
print("Byte-Level BPE í† í¬ë‚˜ì´ì € í•™ìŠµ ì™„ë£Œ! ğŸš€ğŸ”¥")

