from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace

# âœ… í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
tokenizer.pre_tokenizer = Whitespace()

# âœ… í•™ìŠµ ì„¤ì • (í—ˆê¹…í˜ì´ìŠ¤ ìŠ¤íƒ€ì¼)
trainer = BpeTrainer(
    vocab_size=30000,  # ì„œë¸Œì›Œë“œ ê°œìˆ˜
    min_frequency=2,    # ìµœì†Œ ë¹ˆë„
    special_tokens=["[PAD]", "[UNK]", "[CLS]", "[SEP]", "[MASK]"]
)

# âœ… ìœ„í‚¤ ì½”í¼ìŠ¤ì—ì„œ í•™ìŠµ ì§„í–‰
corpus_path = "wiki_corpus.txt"
tokenizer.train([corpus_path], trainer)

# âœ… í† í¬ë‚˜ì´ì € ì €ì¥
tokenizer.save("bpe_tokenizer.json")
print("BPE í† í¬ë‚˜ì´ì € í•™ìŠµ ì™„ë£Œ! ğŸš€ğŸ”¥")
