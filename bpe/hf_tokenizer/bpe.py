from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors, decoders

# 1. ì›ë³¸ í…ìŠ¤íŠ¸ ë°ì´í„° (ê°€ìƒì˜ í…ìŠ¤íŠ¸)
original_texts = [
    "This is LBA 0 data. It is stored in raw binary format.",
    "LBA 1 contains another piece of data. BPE will learn from this.",
    "More data in LBA 2. Tokenization will segment this efficiently.",
    "LBA 3 final data block. BPE encoding applies here as well."
]

# 2. í…ìŠ¤íŠ¸ë¥¼ ë°”ì´ë„ˆë¦¬ë¡œ ë³€í™˜ (NVMe LBA ì €ì¥ ë°©ì‹ê³¼ ìœ ì‚¬)
binary_data = [text.encode('utf-8') for text in original_texts]

# 3. BPE ëª¨ë¸ ìƒì„± (Byte-Level BPE ì ìš©)
tokenizer = Tokenizer(models.BPE())

# 4. Byte-Level Pre-tokenizer ì„¤ì •
tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)

# 5. BPE í•™ìŠµì„ ìœ„í•œ Trainer ì„¤ì • (Byte-Level ì ìš©)
trainer = trainers.BpeTrainer(
    special_tokens=["[UNK]", "[CLS]", "[SEP]", "[PAD]", "[MASK]"], 
    show_progress=True, 
    initial_alphabet=pre_tokenizers.ByteLevel.alphabet()  # Byte ë‹¨ìœ„ í•™ìŠµ
)

# 6. train_from_iterator()ì— binary_dataë¥¼ ë¬¸ìì—´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ í›„ ì „ë‹¬
binary_str_data = [data.decode('utf-8') for data in binary_data]  # ğŸ”¥ ì—¬ê¸°ì„œ ë°”ì´ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
tokenizer.train_from_iterator(binary_str_data, trainer)

# 7. Byte-Level Decoding ì„¤ì •
tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
tokenizer.decoder = decoders.ByteLevel()

# 8. í…ŒìŠ¤íŠ¸ ë¬¸ì¥ (ë°”ì´ë„ˆë¦¬ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ í›„ ì¸ì½”ë”©)
test_sentence = b"LBA tokenization example.".decode('utf-8')  # ğŸ”¥ ì˜¤ë¥˜ í•´ê²°: bytes â†’ str ë³€í™˜
encoded = tokenizer.encode(test_sentence)

# 9. ê²°ê³¼ ì¶œë ¥
print("Encoded Tokens:", encoded.tokens)
print("Decoded Text:", tokenizer.decode(encoded.ids))

